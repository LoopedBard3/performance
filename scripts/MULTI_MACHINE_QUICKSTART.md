# Multi-Machine Partitioning - Quick Start

## ‚úÖ What's New

You can now run the **same CSV** on **multiple machines** without conflicts!

**How it works:**
- Automatic partitioning using consistent hashing
- Each machine processes a different subset of workitems
- No overlap, no duplicates
- Linear speedup (2 machines = 2x faster)

---

## üöÄ Quick Start (2 Machines)

### Step 1: Generate Commands

```bash
python generate_commands.py failed_workitems.csv 2 --workitem-workers 20 --file-workers 10
```

This outputs ready-to-run commands for both machines!

### Step 2: Run on Machine 1

```bash
python reupload_failed_workitems.py \
  --csv failed_workitems.csv \
  --state-db machine1_state.db \
  --partition 0 \
  --total-partitions 2 \
  --workitem-workers 20 \
  --file-workers 10
```

### Step 3: Run on Machine 2

```bash
python reupload_failed_workitems.py \
  --csv failed_workitems.csv \
  --state-db machine2_state.db \
  --partition 1 \
  --total-partitions 2 \
  --workitem-workers 20 \
  --file-workers 10
```

**Done!** Each machine processes ~50% of workitems in parallel.

---

## üìä Performance Impact

### Current (Single Machine - Your Stats)

```
Configuration:          2 workitem workers √ó 25 file workers
Throughput:             0.3 workitems/min
Time for 21k items:     ~7 days
```

### With 2 Machines + Optimized Settings

```
Configuration:          2 machines √ó 20 workitem workers √ó 10 file workers
Throughput:             ~8 workitems/min total (4 wpm per machine)
Time for 21k items:     ~1.8 days
```

**~4x speedup!** (2x from more machines + 2x from better settings)

### With 3 Machines + Optimized Settings

```
Configuration:          3 machines √ó 20 workitem workers √ó 10 file workers
Throughput:             ~12 workitems/min total
Time for 21k items:     ~1.2 days
```

**~6x speedup!**

---

## üõ†Ô∏è Helper Scripts

### 1. `generate_commands.py` - Generate all commands

```bash
# For 2 machines
python generate_commands.py failed_workitems.csv 2

# For 3 machines with custom workers
python generate_commands.py failed_workitems.csv 3 --workitem-workers 30 --file-workers 8

# With --resume flag
python generate_commands.py failed_workitems.csv 2 --resume
```

**Output:** Ready-to-copy commands for all machines!

### 2. `check_partitions.py` - Verify distribution

```bash
python check_partitions.py failed_workitems.csv 2
```

**Output:**
```
Partition Distribution for 2 partitions:
================================================================================

Partition 0:
  Count:     10,523 (50.1%)
  Samples:   1889211721, 1913007684, ...

Partition 1:
  Count:     10,477 (49.9%)
  Samples:   1889211722, 1913007685, ...

Total WorkItems: 21,000
```

Shows exactly which workitems go to each partition!

---

## üìù Important Rules

### ‚úÖ Do This

1. **Same CSV on all machines**
   ```bash
   --csv failed_workitems.csv  # Same file everywhere
   ```

2. **Different state DB per machine**
   ```bash
   # Machine 1
   --state-db machine1.db
   
   # Machine 2
   --state-db machine2.db
   ```

3. **Matching total-partitions**
   ```bash
   # All machines
   --total-partitions 2
   ```

4. **Unique partition number per machine**
   ```bash
   # Machine 1
   --partition 0
   
   # Machine 2
   --partition 1
   ```

### ‚ùå Don't Do This

1. **Shared state database**
   ```bash
   # Both machines
   --state-db shared.db  # SQLite will lock! ‚ùå
   ```

2. **Mismatched total-partitions**
   ```bash
   # Machine 1
   --total-partitions 2
   
   # Machine 2
   --total-partitions 3  # Mismatch! ‚ùå
   ```

3. **Duplicate partition numbers**
   ```bash
   # Machine 1
   --partition 0
   
   # Machine 2
   --partition 0  # Duplicate! ‚ùå
   ```

---

## üîç Monitoring

### On Each Machine

```bash
# Machine 1
python monitor_reupload.py --state-db machine1_state.db

# Machine 2
python monitor_reupload.py --state-db machine2_state.db
```

### Overall Progress (Manual)

```
Total Completed = Machine1 Completed + Machine2 Completed
Total Progress = Total Completed / 21,000
```

---

## üîÑ Resuming

Each machine can resume independently:

```bash
# Machine 1 - resume after crash
python reupload_failed_workitems.py \
  --csv failed_workitems.csv \
  --state-db machine1_state.db \
  --partition 0 \
  --total-partitions 2 \
  --resume  # Just add --resume
```

The partition assignment is the same, so it picks up where it left off!

---

## üí° Recommendations

### For Your Situation (21k workitems, ~1 week estimate)

**Option 1: 2 Machines (Recommended)**
```
Setup:    2 machines √ó 20 workers √ó 10 file workers
Time:     ~1.8 days
Effort:   Low (easy to set up)
```

**Option 2: 3 Machines (Fastest)**
```
Setup:    3 machines √ó 20 workers √ó 10 file workers
Time:     ~1.2 days
Effort:   Medium (need 3 machines)
```

**Option 3: 1 Machine Optimized (Simplest)**
```
Setup:    1 machine √ó 30 workers √ó 8 file workers
Time:     ~2.4 days
Effort:   None (just change parameters)
```

---

## üì¶ What You Need

### On Each Machine

**Files:**
- `failed_workitems.csv` (same CSV everywhere)
- `reupload_failed_workitems.py`
- `upload.py`
- `monitor_reupload.py`

**Python packages:**
```bash
pip install azure-storage-blob azure-storage-queue azure-identity azure-kusto-data
```

**Azure credentials:**
```bash
# Log in on each machine
az login
# Or browser auth will prompt when script runs
```

---

## üéØ Example: Full 2-Machine Setup

### Machine 1 (Windows)

```powershell
# Copy files
copy \\shared\failed_workitems.csv .
copy \\shared\*.py .

# Install packages
pip install azure-storage-blob azure-storage-queue azure-identity azure-kusto-data

# Run
python reupload_failed_workitems.py `
  --csv failed_workitems.csv `
  --state-db machine1.db `
  --partition 0 `
  --total-partitions 2 `
  --workitem-workers 20 `
  --file-workers 10
```

### Machine 2 (Linux)

```bash
# Copy files
scp shared@host:/share/failed_workitems.csv .
scp shared@host:/share/*.py .

# Install packages
pip3 install azure-storage-blob azure-storage-queue azure-identity azure-kusto-data

# Run
python3 reupload_failed_workitems.py \
  --csv failed_workitems.csv \
  --state-db machine2.db \
  --partition 1 \
  --total-partitions 2 \
  --workitem-workers 20 \
  --file-workers 10
```

**Both run in parallel ‚Üí ~1.8 days total!**

---

## üêõ Troubleshooting

### "How do I know which partition a specific workitem is in?"

```bash
python check_partitions.py failed_workitems.csv 2
```

Search for your WorkItemId in the output.

### "One machine finished early"

Normal! Hash distribution isn't perfect. The slower machine will finish soon.

### "Can I add a 3rd machine mid-run?"

**No** - let the current 2 finish, then start fresh with 3 partitions if needed.

### "Machine crashed - can I restart it?"

**Yes!** Use the same partition settings with `--resume`:

```bash
python reupload_failed_workitems.py \
  --csv failed_workitems.csv \
  --state-db machine1.db \
  --partition 0 \
  --total-partitions 2 \
  --resume
```

---

## üìä Summary

| Approach | Machines | Workers/Machine | Total Time | Speedup |
|----------|----------|-----------------|------------|---------|
| Current | 1 | 2 √ó 25 | 7 days | 1x |
| Optimized | 1 | 30 √ó 8 | 2.4 days | 3x |
| **Multi-Machine** | **2** | **20 √ó 10** | **1.8 days** | **4x** |
| Aggressive | 3 | 20 √ó 10 | 1.2 days | 6x |

**Recommended: 2 machines with optimized settings = 4x speedup!** üöÄ

---

## üöÄ Get Started Now

```bash
# Step 1: Generate commands
python generate_commands.py failed_workitems.csv 2 --workitem-workers 20 --file-workers 10

# Step 2: Copy output commands to each machine and run
# Machine 1: Run partition 0 command
# Machine 2: Run partition 1 command

# Step 3: Monitor both
# Machine 1: python monitor_reupload.py --state-db machine1_state.db
# Machine 2: python monitor_reupload.py --state-db machine2_state.db

# Step 4: Wait ~1.8 days instead of 7 days!
```

**That's it!** üéâ

Full details: `MULTI_MACHINE_GUIDE.md`
